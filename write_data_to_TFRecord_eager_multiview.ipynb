{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/me/pratikm/virtualEnv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /misc/me/pratikm/virtualEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "import pickle\n",
    "import time\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = 'data_multiview'\n",
    "DIR_INPUT = os.path.join(DIR_DATA, 'annotated_frames')\n",
    "DIR_OUTPUT = os.path.join(DIR_DATA, 'bounding_boxes')\n",
    "\n",
    "OBJECT_LABELS = {\n",
    "    'hand': (0, 'hand')\n",
    "}\n",
    "NUM_OBJECTS = 1\n",
    "DIM_OUTPUT_PER_GRID_PER_ANCHOR = 5 + NUM_OBJECTS\n",
    "\n",
    "# Reference: https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg#L242 \n",
    "GRID_H, GRID_W = 13, 13 \n",
    "GRID_SIZE = 416//GRID_H\n",
    "ANCHORS = np.array(\n",
    "    [\n",
    "        [0.09112895, 0.06958421],\n",
    "        [0.21102316, 0.16803947],\n",
    "        [0.42625895, 0.26609842],\n",
    "        [0.25476474, 0.49848   ],\n",
    "        [0.52668947, 0.59138947]\n",
    "    ]\n",
    ")\n",
    "NUM_ANCHORS = ANCHORS.shape[0]\n",
    "ANCHORS *= np.array([GRID_H, GRID_W])  # map from [0,1] space to [0,19] space\n",
    "IMG_OUT_H, IMG_OUT_W = GRID_H * GRID_SIZE, GRID_W * GRID_SIZE \n",
    "\n",
    "DIR_TFRECORDS = DIR_DATA + '/' + 'data_hand_multiview_tfrecords'\n",
    "NUM_EXAMPLES_PER_TFRECORD = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose2corner(pose):\n",
    "    # pose.shape = [21, 2] in uv (image) space\n",
    "    xy_min = np.min(pose, axis=0)\n",
    "    xy_max = np.max(pose, axis=0)\n",
    "    \n",
    "    return np.array([xy_min[1], xy_min[0], xy_max[1], xy_max[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "def normalize_data(img, target):\n",
    "    # resize input\n",
    "    img_in_h = img.shape[0]\n",
    "    img_in_w = img.shape[1]\n",
    "    img = cv2.resize(img, (IMG_OUT_W, IMG_OUT_H))\n",
    "    \n",
    "    # get corners\n",
    "    y_min = target[0]\n",
    "    x_min = target[1]\n",
    "    y_max = target[2]\n",
    "    x_max = target[3]\n",
    "    \n",
    "    # convert from corner coordinates to x_center, y_center, width, height\n",
    "    y_center, x_center = (y_min + y_max)/2., (x_min + x_max)/2.\n",
    "    bbox_h, bbox_w = y_max - y_min, x_max - x_min\n",
    "\n",
    "    # normalize these values s.t. image goes from 0 to 1 (helps for arbitary size image size)\n",
    "    y_center /= img_in_h\n",
    "    x_center /= img_in_w\n",
    "    bbox_h /= img_in_h\n",
    "    bbox_w /= img_in_w\n",
    "    \n",
    "    class_idx = int(OBJECT_LABELS['hand'][0])\n",
    "    target_normalized = np.array([y_center, x_center, bbox_h, bbox_w, class_idx], dtype=np.float32) \n",
    "    \n",
    "    \n",
    "    return img, target_normalized\n",
    "\n",
    "def get_iou(hw1, hw2):\n",
    "    # hw: (height, width)\n",
    "    # assumption: both boxes have same centers\n",
    "    \n",
    "    # get extremes of both boxes\n",
    "    hw1_max, hw2_max = hw1/2., hw2/2.\n",
    "    hw1_min, hw2_min = -hw1_max, -hw2_max\n",
    "    \n",
    "    # get intersection area\n",
    "    intersection_min = np.maximum(hw1_min, hw2_min)\n",
    "    intersection_max = np.minimum(hw1_max, hw2_max)\n",
    "    hw_intersection = np.maximum(intersection_max-intersection_min, 0.)\n",
    "    area_intersection = hw_intersection[0] * hw_intersection[1]\n",
    "    \n",
    "    # get union area\n",
    "    area_hw1 = hw1[0] * hw1[1]\n",
    "    area_hw2 = hw2[0] * hw2[1]\n",
    "    area_union = area_hw1 + area_hw2 - area_intersection\n",
    "    \n",
    "    iou = area_intersection / area_union\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def target2label(target):\n",
    "    # initialize return data\n",
    "    label = np.zeros((GRID_H, GRID_W, NUM_ANCHORS, 6), dtype=np.float32)  # 6: [offset_y, offset_x, scale_h, scale_w, class_idx, prob_obj]\n",
    "    \n",
    "    target_class = target[4]\n",
    "\n",
    "    # map bbox from [0,1] space to [0,19] space\n",
    "    bbox = target[0:4] * np.array([GRID_H, GRID_W, GRID_H, GRID_W])\n",
    "\n",
    "    # get grid index for bbox center\n",
    "    idx_y = int(floor(bbox[0]))\n",
    "    idx_x = int(floor(bbox[1]))\n",
    "    \n",
    "    # find best anchor corresponding to bbox\n",
    "    iou_best, idx_anchor_best = 0., 0\n",
    "    for idx_anchor, anchor in enumerate(ANCHORS):\n",
    "        iou = get_iou(bbox[2:4], anchor)\n",
    "        if iou > iou_best:\n",
    "            iou_best = iou\n",
    "            idx_anchor_best = idx_anchor\n",
    "\n",
    "    # update label\n",
    "    if iou_best > 0.:\n",
    "        label[idx_y, idx_x, idx_anchor_best] = np.array(\n",
    "            [\n",
    "                bbox[0] - idx_y,  # offset of box_center from top-left corner of grid containing box_center\n",
    "                bbox[1] - idx_x,\n",
    "                bbox[2]/ANCHORS[idx_anchor_best,0], # scale of anchor box so as to fit the bbox\n",
    "                bbox[3]/ANCHORS[idx_anchor_best,1],\n",
    "                target_class,\n",
    "                1.0  # prob_object (object is present with prob=1)\n",
    "            ], dtype=np.float32\n",
    "        )\n",
    "    return label\n",
    "        \n",
    "def get_processed_data(img, target):\n",
    "    # normalize input and output\n",
    "    img, target = normalize_data(img, target)\n",
    "    # target.shape = (1, 5)\n",
    "    # 5 corresponds to (c_y, c_x, h, w, class_idx)\n",
    "    \n",
    "    label = target2label(target)\n",
    "    \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion functions (data to feature data types)\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def write_example_to_TFRecord(img, target, writer):\n",
    "    # get processed data\n",
    "    img, label = get_processed_data(img, target)\n",
    "    \n",
    "    # create example from this data\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'img': _bytes_feature(img.tostring()),\n",
    "                'label': _bytes_feature(label.tostring()) \n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "# for each camera in each data sequence\n",
    "def write_data_to_TFRecord(img_paths, target_paths, out_dir):\n",
    "    # write data into multiple TFRecord files\n",
    "    idx_tfrecord, idx_data = 0, 0\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    \n",
    "    while idx_data < len(img_paths):\n",
    "        # new TFRecord file\n",
    "        filename_tfrecord = os.path.join(out_dir, str(idx_tfrecord) + '.tfrecords')\n",
    "        with tf.python_io.TFRecordWriter(filename_tfrecord) as writer:\n",
    "            # write examples into this file until limit is reached\n",
    "            idx_example = 0\n",
    "            while idx_data < len(img_paths) and idx_example < NUM_EXAMPLES_PER_TFRECORD:\n",
    "                path = img_paths[idx_data]\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                target = np.loadtxt(target_paths[idx_data], usecols=1)\n",
    "                idx_data += 1\n",
    "                \n",
    "                write_example_to_TFRecord(img, target, writer)\n",
    "                idx_example += 1\n",
    "            idx_tfrecord += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 13 is out of bounds for axis 1 with size 13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4d59d479437f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mwrite_data_to_TFRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7a8511a78d03>\u001b[0m in \u001b[0;36mwrite_data_to_TFRecord\u001b[0;34m(img_paths, target_paths, out_dir)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0midx_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mwrite_example_to_TFRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0midx_example\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0midx_tfrecord\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7a8511a78d03>\u001b[0m in \u001b[0;36mwrite_example_to_TFRecord\u001b[0;34m(img, target, writer)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_example_to_TFRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# get processed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_processed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# create example from this data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cb28c1d98416>\u001b[0m in \u001b[0;36mget_processed_data\u001b[0;34m(img, target)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# 5 corresponds to (c_y, c_x, h, w, class_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cb28c1d98416>\u001b[0m in \u001b[0;36mtarget2label\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mtarget_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;36m1.0\u001b[0m  \u001b[0;31m# prob_object (object is present with prob=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             ], dtype=np.float32\n\u001b[0m\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 13 is out of bounds for axis 1 with size 13"
     ]
    }
   ],
   "source": [
    "data_dirs = natsorted(os.listdir(DIR_INPUT))\n",
    "for data_dir in data_dirs:\n",
    "    for idx_cam in range(4):  # for 4 views\n",
    "        input_path = os.path.join(DIR_INPUT, data_dir)\n",
    "        img_names = os.listdir(input_path)\n",
    "        pattern = 'webcam_'+str(idx_cam+1)\n",
    "        img_names = natsorted(list(filter(lambda name: pattern in name, img_names)))\n",
    "        img_paths = [os.path.join(input_path, name) for name in img_names]\n",
    "        \n",
    "        label_path = os.path.join(DIR_OUTPUT, data_dir)\n",
    "        label_names = os.listdir(label_path)\n",
    "        pattern = 'bbox_'+str(idx_cam+1)\n",
    "        label_names = natsorted(list(filter(lambda name: pattern in name, label_names)))\n",
    "        label_paths = [os.path.join(label_path, name) for name in label_names]\n",
    "        \n",
    "        out_dir = os.path.join(DIR_TFRECORDS, data_dir)\n",
    "        out_dir = os.path.join(out_dir, str(idx_cam))\n",
    "    \n",
    "        write_data_to_TFRecord(img_paths, label_paths, out_dir)\n",
    "        \n",
    "        print('Finished', data_dir, idx_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
