{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "NUM_OBJECTS = 1\n",
    "MAX_DETECTIONS_PER_IMAGE = 1\n",
    "GRID_H, GRID_W = 13, 13\n",
    "GRID_SIZE = 416//GRID_H \n",
    "ANCHORS_NORMALIZED = np.array(\n",
    "    [\n",
    "        [0.05210654, 0.04405615],\n",
    "        [0.15865615, 0.14418923],\n",
    "        [0.42110308, 0.25680231],\n",
    "        [0.27136769, 0.60637077],\n",
    "        [0.70525231, 0.75157846]\n",
    "    ]\n",
    ")\n",
    "ANCHORS = ANCHORS_NORMALIZED * np.array([GRID_H, GRID_W])\n",
    "NUM_ANCHORS = ANCHORS.shape[0]\n",
    "IMG_H, IMG_W = GRID_H * GRID_SIZE, GRID_W * GRID_SIZE\n",
    "THRESHOLD_OUT_PROB = 0.6\n",
    "THRESHOLD_IOU_NMS = 0.5\n",
    "\n",
    "VIDEO_NAME = 'VID_20180429_173507.mp4'\n",
    "CHECKPOINT_DIR = 'model_hand_detection'\n",
    "DIR_OUT = 'imgs_out'\n",
    "\n",
    "if not os.path.exists(DIR_OUT):\n",
    "        os.makedirs(DIR_OUT)\n",
    "\n",
    "if tfe.num_gpus() > 0:\n",
    "    DEVICE = '/gpu:0'\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    DEVICE = '/cpu:0'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(predictions):\n",
    "    predictions_yx = tf.sigmoid(predictions[..., 0:2])\n",
    "    predictions_hw = tf.exp(predictions[...,2:4])\n",
    "    predictions_prob_obj = tf.sigmoid(predictions[...,4:5])\n",
    "    predictions_prob_class = tf.nn.softmax(predictions[...,5:])\n",
    "    \n",
    "    return predictions_yx, predictions_hw, predictions_prob_obj, predictions_prob_class\n",
    "\n",
    "def get_coordinates(h, w):\n",
    "    coordinates_y = tf.range(h)\n",
    "    coordinates_x = tf.range(w)\n",
    "    x, y = tf.meshgrid(coordinates_x, coordinates_y)\n",
    "    coordinates = tf.stack([y, x], axis=-1)\n",
    "    coordinates = tf.reshape(coordinates, [1, h, w, 1, 2])\n",
    "    coordinates = tf.cast(coordinates, tf.float32)\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "def grid2normalized(predictions_yx, predictions_hw):    \n",
    "    # create cartesian coordinates on grid space\n",
    "    coordinates = get_coordinates(GRID_H, GRID_W)\n",
    "    \n",
    "    # map from grid space to [0,19] space\n",
    "    anchors = tf.cast(tf.reshape(ANCHORS, [1, 1, 1, ANCHORS.shape[0], 2]), dtype=tf.float32)  # [0,19] space\n",
    "    predictions_yx += coordinates\n",
    "    predictions_hw *= anchors\n",
    "    \n",
    "    # map from [0,19] space to [0,1] space\n",
    "    shape = tf.cast(tf.reshape([GRID_H, GRID_W], [1, 1, 1, 1, 2]), tf.float32)\n",
    "    predictions_yx /= shape\n",
    "    predictions_hw /= shape\n",
    "    \n",
    "    return predictions_yx, predictions_hw\n",
    "\n",
    "def center2corner(predictions_yx, predictions_hw):\n",
    "    # predictions_yx = [GRID_H, GRID_W, NUM_ANCHORS, 2]\n",
    "    \n",
    "    bbox_min = predictions_yx - (predictions_hw/2.)\n",
    "    bbox_max = predictions_yx + (predictions_hw/2.)\n",
    "    \n",
    "    predictions_corner = tf.concat([bbox_min[...,0:1], bbox_min[...,1:2], bbox_max[...,0:1], bbox_max[...,1:2]], axis=-1)\n",
    "    \n",
    "    return predictions_corner\n",
    "\n",
    "def get_filtered_predictions(predictions_corner, predictions_prob_obj, predictions_prob_class):\n",
    "    # compute overall prob for each anchor in each grid\n",
    "    predictions_prob = predictions_prob_obj * predictions_prob_class\n",
    "    \n",
    "    # get max prob among all classes at each anchor in each grid\n",
    "    predictions_idx_class_max = tf.argmax(predictions_prob, axis=-1)\n",
    "    predictions_prob = tf.reduce_max(predictions_prob, axis=-1)\n",
    "    \n",
    "    # compute filter mask\n",
    "    mask_filter = predictions_prob >= THRESHOLD_OUT_PROB\n",
    "    \n",
    "    # apply mask on output\n",
    "    bbox_filtered = tf.boolean_mask(predictions_corner, mask_filter)\n",
    "    prob_filtered = tf.boolean_mask(predictions_prob, mask_filter)\n",
    "    with tf.device('/cpu:0'):\n",
    "        idx_class_filtered = tf.boolean_mask(predictions_idx_class_max, mask_filter)\n",
    "    \n",
    "    if DEVICE == '/gpu:0':\n",
    "        idx_class_filtered = idx_class_filtered.gpu()        \n",
    "    \n",
    "    return bbox_filtered, prob_filtered, idx_class_filtered\n",
    "\n",
    "\n",
    "def predictions2outputs(predictions):\n",
    "    # apply corresponding transformations on predictions\n",
    "    predictions_yx, predictions_hw, predictions_prob_obj, predictions_prob_class = apply_transformations(predictions)\n",
    "    \n",
    "    # map predictions_bbox to [0,1] space\n",
    "    predictions_yx, predictions_hw = grid2normalized(predictions_yx, predictions_hw)\n",
    "    \n",
    "    # represent boxes using corners\n",
    "    predictions_corner = center2corner(predictions_yx, predictions_hw)\n",
    "    \n",
    "    # filter predictions based on (prob_obj * prob_class). (needs to be done separately for each image in batch)\n",
    "    bbox_filtered, prob_filtered, idx_class_filtered = get_filtered_predictions(predictions_corner, predictions_prob_obj, predictions_prob_class)\n",
    "    # bbox_filtered.shape = [BATCH_SIZE, NUM_FILTERED, 4]\n",
    "    \n",
    "    # TODO: perform nms for each class separately\n",
    "    # scale boxes from [0,1] to image space\n",
    "    img_space = tf.reshape(tf.cast(tf.stack([IMG_H, IMG_W, IMG_H, IMG_W]), tf.float32), [1, 1, 4])\n",
    "    bbox_filtered = tf.reshape(bbox_filtered*img_space, [-1, 4])  # tf.nms takes num_boxes (no batch support)\n",
    "    \n",
    "    # perform non-max suppression\n",
    "    with tf.device('/cpu:0'):\n",
    "        bbox_nms_indices = tf.image.non_max_suppression(bbox_filtered, tf.reshape(prob_filtered,[-1]), MAX_DETECTIONS_PER_IMAGE, THRESHOLD_IOU_NMS)\n",
    "    if DEVICE == '/gpu:0':\n",
    "        bbox_nms_indices = bbox_nms_indices.gpu()\n",
    "    \n",
    "    bbox_nms = tf.gather(bbox_filtered, bbox_nms_indices)  # box_nms.shape = [len(bbox_nms_indices), 4]\n",
    "    prob_nms = tf.expand_dims(tf.gather(prob_filtered, bbox_nms_indices), axis=-1) # prob_nms.shape = [len(bbox_nms_indices), 1]\n",
    "    with tf.device('/cpu:0'):\n",
    "        idx_class_nms = tf.expand_dims(tf.cast(tf.gather(idx_class_filtered, bbox_nms_indices), tf.float32), axis=-1)\n",
    "    if DEVICE == '/gpu:0':\n",
    "        idx_class_nms = idx_class_nms.gpu()\n",
    "    \n",
    "    # concat return data\n",
    "    output = tf.concat([bbox_nms, prob_nms, idx_class_nms], axis=-1)\n",
    "\n",
    "    return tf.expand_dims(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on video\n",
    "def process_img(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_W, IMG_H))\n",
    "    img = (img / 255.).astype(np.float32)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def draw_output(img, output):\n",
    "    # unnormalize image\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "    output = output.astype(np.int32)\n",
    "    for idx_box in range(output.shape[0]):\n",
    "        bbox = output[idx_box]\n",
    "        img = cv2.rectangle(img, (bbox[1], bbox[0]), (bbox[3], bbox[2]), color=(255, 0, 0), thickness=3)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        \n",
    "        # add layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False)\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False)\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False)\n",
    "        self.norm3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv4 = tf.keras.layers.Conv2D(64, 1, padding='same', use_bias=False)\n",
    "        self.norm4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv5 = tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False)\n",
    "        self.norm5 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool5 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv6 = tf.keras.layers.Conv2D(256, 3, padding='same', use_bias=False)\n",
    "        self.norm6 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv7 = tf.keras.layers.Conv2D(128, 1, padding='same', use_bias=False)\n",
    "        self.norm7 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv8 = tf.keras.layers.Conv2D(256, 3, padding='same', use_bias=False)\n",
    "        self.norm8 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool8 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv9 = tf.keras.layers.Conv2D(512, 3, padding='same', use_bias=False)\n",
    "        self.norm9 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv10 = tf.keras.layers.Conv2D(256, 1, padding='same', use_bias=False)\n",
    "        self.norm10 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv11 = tf.keras.layers.Conv2D(512, 3, padding='same', use_bias=False)\n",
    "        self.norm11 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv12 = tf.keras.layers.Conv2D(256, 1, padding='same', use_bias=False)\n",
    "        self.norm12 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv13 = tf.keras.layers.Conv2D(512, 3, padding='same', use_bias=False)\n",
    "        self.norm13 = tf.keras.layers.BatchNormalization()  # skip after this\n",
    "        self.pool13 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv14 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm14 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv15 = tf.keras.layers.Conv2D(512, 1, padding='same', use_bias=False)\n",
    "        self.norm15 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv16 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm16 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv17 = tf.keras.layers.Conv2D(512, 1, padding='same', use_bias=False)\n",
    "        self.norm17 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv18 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm18 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv19 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm19 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv20 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm20 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv21 = tf.keras.layers.Conv2D(64, 1, padding='same', use_bias=False)  # apply on skipped connection\n",
    "        self.norm21 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv22 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm22 = tf.keras.layers.BatchNormalization()\n",
    "        # Feature Extractor Ends Here!\n",
    "        \n",
    "        # Detector Layer!\n",
    "        self.conv23 = tf.keras.layers.Conv2D(NUM_ANCHORS*(4+1+NUM_OBJECTS), 1, padding='same')\n",
    "    \n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # imgs.shape = [B, IMG_H, IMG_W, 3]\n",
    "        \n",
    "        # for now, resize and reshape imgs to vector\n",
    "        imgs = tf.image.resize_images(imgs, [416, 416])\n",
    "        \n",
    "        x = self.conv1(imgs)\n",
    "        x = self.norm1(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.norm5(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.norm6(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.norm7(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = self.norm8(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool8(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = self.norm9(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv10(x)\n",
    "        x = self.norm10(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = self.norm11(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv12(x)\n",
    "        x = self.norm12(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv13(x)\n",
    "        x = self.norm13(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x_skip = tf.identity(x)\n",
    "        x = self.pool13(x)\n",
    "        \n",
    "        x = self.conv14(x)\n",
    "        x = self.norm14(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv15(x)\n",
    "        x = self.norm15(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv16(x)\n",
    "        x = self.norm16(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv17(x)\n",
    "        x = self.norm17(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv18(x)\n",
    "        x = self.norm18(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv19(x)\n",
    "        x = self.norm19(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv20(x)\n",
    "        x = self.norm20(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x_skip = self.conv21(x_skip)\n",
    "        x_skip = self.norm21(x_skip)\n",
    "        x_skip = tf.nn.leaky_relu(x_skip, alpha=0.1)\n",
    "        x_skip = tf.space_to_depth(x_skip, block_size=2)  # lossless shrinkage of feature map\n",
    "        \n",
    "        x = tf.concat([x_skip, x], axis=-1)  # low_level features concatenated with high_level features\n",
    "        \n",
    "        x = self.conv22(x)\n",
    "        x = self.norm22(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        # Feature Extractor ends here!\n",
    "        \n",
    "        # Detector layer\n",
    "        x = self.conv23(x)\n",
    "        \n",
    "        # reshape output\n",
    "        pred = tf.reshape(x, [-1, GRID_H, GRID_W, NUM_ANCHORS, 4+1+NUM_OBJECTS])\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "    def predict(self, imgs):\n",
    "        '''predicts bboxes and draws them on the image'''\n",
    "        # imgs.shape = [B, IMG_H, IMG_W, 3]\n",
    "        \n",
    "        # forward pass\n",
    "        predictions = self.forward(imgs)\n",
    "        \n",
    "        # swap x & y and also h & w\n",
    "        predictions = tf.concat([predictions[...,1::-1], predictions[...,3:1:-1], predictions[...,4:]], axis=-1)\n",
    "        \n",
    "        # post-process to get bounding boxes\n",
    "        outputs = predictions2outputs(predictions)  \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    model = Model()\n",
    "    checkpoint = tfe.Checkpoint(model=model, optimizer_step=tf.train.get_or_create_global_step())\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    vidcap = cv2.VideoCapture(VIDEO_NAME)\n",
    "    success,img = vidcap.read()\n",
    "    success = True\n",
    "    idx_img = 0\n",
    "    while success:\n",
    "        img = process_img(img)\n",
    "\n",
    "        output = model.predict(img)\n",
    "\n",
    "        # write images\n",
    "        img_out = draw_output(img[0], output[0].numpy())\n",
    "        cv2.imwrite(DIR_OUT+ '/'+str(idx_img) + '.jpg', cv2.cvtColor(img_out, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        success,img = vidcap.read()\n",
    "        idx_img += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
