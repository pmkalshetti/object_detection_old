{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/me/pratikm/virtualEnv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /misc/me/pratikm/virtualEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TFRECORDS = 'data_multiview'\n",
    "DATA_TRAIN = natsorted(glob('./'+DIR_TFRECORDS+'/**/*.tfrecords', recursive=True))\n",
    "\n",
    "NUM_OBJECTS = 1\n",
    "MAX_DETECTIONS_PER_IMAGE = 1\n",
    "\n",
    "GRID_H, GRID_W = 13, 13\n",
    "GRID_SIZE = 416//GRID_H \n",
    "\n",
    "ANCHORS_NORMALIZED = np.array(\n",
    "    [\n",
    "        [0.05210654, 0.04405615],\n",
    "        [0.15865615, 0.14418923],\n",
    "        [0.42110308, 0.25680231],\n",
    "        [0.27136769, 0.60637077],\n",
    "        [0.70525231, 0.75157846]\n",
    "    ]\n",
    ")\n",
    "\n",
    "ANCHORS = ANCHORS_NORMALIZED * np.array([GRID_H, GRID_W])\n",
    "NUM_ANCHORS = ANCHORS.shape[0]\n",
    "\n",
    "IMG_H, IMG_W = GRID_H * GRID_SIZE, GRID_W * GRID_SIZE\n",
    "\n",
    "THRESHOLD_IOU_SCORES = 0.6\n",
    "COEFF_LOSS_CONFIDENCE_OBJECT_PRESENT = 5\n",
    "COEFF_LOSS_CONFIDENCE_OBJECT_ABSENT = 1\n",
    "THRESHOLD_OUT_PROB = 0.0001\n",
    "THRESHOLD_IOU_NMS = 0.5\n",
    "\n",
    "NUM_EPOCHS = 10 # not used\n",
    "BATCH_SIZE = 16\n",
    "CHECKPOINT_DIR = 'model'\n",
    "CHECKPOINT_PREFIX = os.path.join(CHECKPOINT_DIR, \"ckpt\")\n",
    "DIR_IMG_OUT = 'imgs_out'\n",
    "NUM_BATCHES = 4068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(record):\n",
    "    # dictionary as per saved TFRecord\n",
    "    keys_to_features = {\n",
    "        'img': tf.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "        'label': tf.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "    }\n",
    "\n",
    "    # parse record\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    # decode image\n",
    "    img = tf.decode_raw(parsed['img'], tf.uint8)\n",
    "    img = tf.cast(tf.reshape(img, [IMG_H, IMG_W, 3]), tf.float32)\n",
    "    img /= 255.  # normalize\n",
    "\n",
    "    # decode label\n",
    "    label = tf.decode_raw(parsed['label'], tf.float32)\n",
    "    label = tf.reshape(label, [GRID_H, GRID_W, NUM_ANCHORS, 6])\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_output(img, output):\n",
    "    # unnormalize image\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "    for idx_box in range(output.shape[0]):\n",
    "        conf = output[idx_box][4]\n",
    "        bbox = output[idx_box].astype(np.int32)\n",
    "        img = cv2.rectangle(img, (bbox[1], bbox[0]), (bbox[3], bbox[2]), color=(255, 0, 0), thickness=3)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        img = cv2.putText(img, '{:.2f}'.format(conf),(bbox[1], bbox[0]), font, .5,(0,255,255),2,cv2.LINE_AA)\n",
    "        \n",
    "    return img\n",
    "\n",
    "def center2corner(predictions_yx, predictions_hw):\n",
    "    # predictions_yx = [GRID_H, GRID_W, NUM_ANCHORS, 2]\n",
    "    \n",
    "    bbox_min = predictions_yx - (predictions_hw/2.)\n",
    "    bbox_max = predictions_yx + (predictions_hw/2.)\n",
    "    \n",
    "    predictions_corner = tf.concat([bbox_min[...,0:1], bbox_min[...,1:2], bbox_max[...,0:1], bbox_max[...,1:2]], axis=-1)\n",
    "    \n",
    "    return predictions_corner\n",
    "\n",
    "def get_filtered_predictions(predictions_corner, predictions_prob_obj, predictions_prob_class):\n",
    "    # compute overall prob for each anchor in each grid\n",
    "    predictions_prob = predictions_prob_obj * predictions_prob_class\n",
    "    \n",
    "    # get max prob among all classes at each anchor in each grid\n",
    "    predictions_idx_class_max = tf.argmax(predictions_prob, axis=-1)\n",
    "    predictions_prob = tf.reduce_max(predictions_prob, axis=-1)\n",
    "    \n",
    "    # compute filter mask\n",
    "    mask_filter = predictions_prob >= THRESHOLD_OUT_PROB\n",
    "    \n",
    "    # apply mask on output\n",
    "    bbox_filtered = tf.boolean_mask(predictions_corner, mask_filter)\n",
    "    prob_filtered = tf.boolean_mask(predictions_prob, mask_filter)\n",
    "    with tf.device('/cpu:0'):\n",
    "        idx_class_filtered = tf.boolean_mask(predictions_idx_class_max, mask_filter)\n",
    "    \n",
    "    return bbox_filtered, prob_filtered, idx_class_filtered\n",
    "\n",
    "\n",
    "def predictions2outputs(predictions):\n",
    "    # apply corresponding transformations on predictions\n",
    "    predictions_yx, predictions_hw, predictions_prob_obj, predictions_prob_class = apply_transformations(predictions)\n",
    "    \n",
    "    # map predictions_bbox to [0,1] space\n",
    "    predictions_yx, predictions_hw = grid2normalized(predictions_yx, predictions_hw)\n",
    "    \n",
    "    # represent boxes using corners\n",
    "    predictions_corner = center2corner(predictions_yx, predictions_hw)\n",
    "    \n",
    "    # filter predictions based on (prob_obj * prob_class). (needs to be done separately for each image in batch)\n",
    "    bbox_filtered, prob_filtered, idx_class_filtered = get_filtered_predictions(predictions_corner, predictions_prob_obj, predictions_prob_class)\n",
    "    # bbox_filtered.shape = [BATCH_SIZE, NUM_FILTERED, 4]\n",
    "    \n",
    "    # TODO: perform nms for each class separately\n",
    "    # scale boxes from [0,1] to image space\n",
    "    img_space = tf.reshape(tf.cast(tf.stack([IMG_H, IMG_W, IMG_H, IMG_W]), tf.float32), [1, 1, 4])\n",
    "    bbox_filtered = tf.reshape(bbox_filtered*img_space, [-1, 4])  # tf.nms takes num_boxes (no batch support)\n",
    "    \n",
    "    # perform non-max suppression\n",
    "    with tf.device('/cpu:0'):\n",
    "        bbox_nms_indices = tf.image.non_max_suppression(bbox_filtered, tf.reshape(prob_filtered,[-1]), MAX_DETECTIONS_PER_IMAGE)\n",
    "    bbox_nms = tf.gather(bbox_filtered, bbox_nms_indices)  # box_nms.shape = [len(bbox_nms_indices), 4]\n",
    "    prob_nms = tf.expand_dims(tf.gather(prob_filtered, bbox_nms_indices), axis=-1) # prob_nms.shape = [len(bbox_nms_indices), 1]\n",
    "    with tf.device('/cpu:0'):\n",
    "        idx_class_nms = tf.expand_dims(tf.cast(tf.gather(idx_class_filtered, bbox_nms_indices), tf.float32), axis=-1)\n",
    "    \n",
    "    # concat return data\n",
    "    output = tf.concat([bbox_nms, prob_nms, idx_class_nms], axis=-1)\n",
    "\n",
    "    return tf.expand_dims(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(predictions):\n",
    "    predictions_yx = tf.sigmoid(predictions[..., 0:2])\n",
    "    predictions_hw = tf.exp(predictions[...,2:4])\n",
    "    predictions_prob_obj = tf.sigmoid(predictions[...,4:5])\n",
    "    predictions_prob_class = tf.nn.softmax(predictions[...,5:])\n",
    "    \n",
    "    return predictions_yx, predictions_hw, predictions_prob_obj, predictions_prob_class\n",
    "\n",
    "def get_coordinates(h, w):\n",
    "    coordinates_y = tf.range(h)\n",
    "    coordinates_x = tf.range(w)\n",
    "    x, y = tf.meshgrid(coordinates_x, coordinates_y)\n",
    "    coordinates = tf.stack([y, x], axis=-1)\n",
    "    coordinates = tf.reshape(coordinates, [1, h, w, 1, 2])\n",
    "    coordinates = tf.cast(coordinates, tf.float32)\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "def grid2normalized(predictions_yx, predictions_hw):    \n",
    "    # create cartesian coordinates on grid space\n",
    "    coordinates = get_coordinates(GRID_H, GRID_W)\n",
    "    \n",
    "    # map from grid space to [0,19] space\n",
    "    anchors = tf.cast(tf.reshape(ANCHORS, [1, 1, 1, ANCHORS.shape[0], 2]), dtype=tf.float32)  # [0,19] space\n",
    "    predictions_yx += coordinates\n",
    "    predictions_hw *= anchors\n",
    "    \n",
    "    # map from [0,19] space to [0,1] space\n",
    "    shape = tf.cast(tf.reshape([GRID_H, GRID_W], [1, 1, 1, 1, 2]), tf.float32)\n",
    "    predictions_yx /= shape\n",
    "    predictions_hw /= shape\n",
    "    \n",
    "    return predictions_yx, predictions_hw\n",
    "\n",
    "def get_boxes_gt(args_map):\n",
    "    # extract ground truth bboxes wherever prob_obj = 1\n",
    "    mask_object = tf.cast(tf.reshape(args_map[1], [GRID_H, GRID_W, NUM_ANCHORS]), tf.bool)\n",
    "    bboxes = tf.boolean_mask(args_map[0], mask_object)\n",
    "    # bboxes.shape = [NUM_DETECTIONS, 4]; NUM_DETECTIONS vary with each image\n",
    "    \n",
    "    # pad bboxes so that bboxes is fixed dimension (fix NUM_DETECTIONS to MAX_DETECTIONS_PER_IMAGE)\n",
    "    pad = tf.zeros((MAX_DETECTIONS_PER_IMAGE - tf.shape(bboxes)[0], 4))  # TODO: when NUM_DETECTIONS > MAX_DETECTIONS_PER_IMAGE\n",
    "    bboxes = tf.concat([bboxes, pad], axis=0)\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def get_iou_scores(predictions_yx, predictions_hw, bboxes_gt):\n",
    "    # predictions_yx.shape = predictions_hw.shape = [BATCH_SIZE, GRID_H, GRID_W, NUM_ANCHORS, 2]\n",
    "    # bboxes_gt.shape = [BATCH_SIZE, MAX_DETECTIONS_PER_IMAGE, 4]\n",
    "    \n",
    "    # compute ious for each anchor in each grid in axis=4\n",
    "    predictions_yx = tf.expand_dims(predictions_yx, 4)\n",
    "    predictions_hw = tf.expand_dims(predictions_hw, 4)\n",
    "    \n",
    "    predictions_min = predictions_yx - predictions_hw/2.\n",
    "    predictions_max = predictions_yx + predictions_hw/2.\n",
    "    \n",
    "    bboxes_gt = tf.reshape(bboxes_gt, [tf.shape(bboxes_gt)[0], 1, 1, 1, MAX_DETECTIONS_PER_IMAGE, 4])\n",
    "    bboxes_gt_yx = bboxes_gt[..., 0:2]\n",
    "    bboxes_gt_hw = bboxes_gt[..., 2:4]\n",
    "    \n",
    "    bboxes_gt_min = bboxes_gt_yx - bboxes_gt_hw/2.\n",
    "    bboxes_gt_max = bboxes_gt_yx + bboxes_gt_hw/2.\n",
    "    \n",
    "    intersection_min = tf.maximum(predictions_min, bboxes_gt_min)\n",
    "    intersection_max = tf.minimum(predictions_max, bboxes_gt_max)\n",
    "    intersection_hw = tf.maximum(intersection_max - intersection_min, 0.)\n",
    "    area_intersection = intersection_hw[..., 0] * intersection_hw[..., 1]\n",
    "    \n",
    "    area_predictions = predictions_hw[...,0] * predictions_hw[...,1]\n",
    "    area_bboxes_gt = bboxes_gt_hw[...,0] * bboxes_gt_hw[...,1]\n",
    "    area_union = area_bboxes_gt + area_predictions - area_intersection\n",
    "    iou = area_intersection / area_union\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def get_confidence_loss(labels_prob_obj, iou_mask, predictions_prob_obj):\n",
    "    mask_object_absent = (1 - labels_prob_obj) * (1 - iou_mask)\n",
    "    loss_object_absent = mask_object_absent * tf.square(predictions_prob_obj)\n",
    "    \n",
    "    loss_object_present = labels_prob_obj * tf.square(1-predictions_prob_obj)\n",
    "    \n",
    "    loss_confidence = COEFF_LOSS_CONFIDENCE_OBJECT_ABSENT * loss_object_absent \\\n",
    "            + COEFF_LOSS_CONFIDENCE_OBJECT_PRESENT * loss_object_present\n",
    "    \n",
    "    return tf.reduce_sum(loss_confidence)\n",
    "    \n",
    "def get_classification_loss(labels_prob_obj, labels_class, predictions_prob_class):\n",
    "    labels_class = tf.cast(labels_class, tf.int32)\n",
    "    labels_class = tf.one_hot(labels_class, NUM_OBJECTS)\n",
    "    \n",
    "    loss_classification = labels_prob_obj * tf.squared_difference(labels_class, predictions_prob_class)\n",
    "    \n",
    "    return tf.reduce_sum(loss_classification)\n",
    "\n",
    "def get_regression_loss(labels_bbox, predictions_bbox, labels_prob_obj):\n",
    "    loss_regression = labels_prob_obj * tf.squared_difference(labels_bbox,predictions_bbox)\n",
    "    \n",
    "    return tf.reduce_sum(loss_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        \n",
    "        # add layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False)\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False)\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False)\n",
    "        self.norm3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv4 = tf.keras.layers.Conv2D(64, 1, padding='same', use_bias=False)\n",
    "        self.norm4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv5 = tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False)\n",
    "        self.norm5 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool5 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv6 = tf.keras.layers.Conv2D(256, 3, padding='same', use_bias=False)\n",
    "        self.norm6 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv7 = tf.keras.layers.Conv2D(128, 1, padding='same', use_bias=False)\n",
    "        self.norm7 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv8 = tf.keras.layers.Conv2D(256, 3, padding='same', use_bias=False)\n",
    "        self.norm8 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool8 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv9 = tf.keras.layers.Conv2D(512, 3, padding='same', use_bias=False)\n",
    "        self.norm9 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv10 = tf.keras.layers.Conv2D(256, 1, padding='same', use_bias=False)\n",
    "        self.norm10 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv11 = tf.keras.layers.Conv2D(512, 3, padding='same', use_bias=False)\n",
    "        self.norm11 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv12 = tf.keras.layers.Conv2D(256, 1, padding='same', use_bias=False)\n",
    "        self.norm12 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv13 = tf.keras.layers.Conv2D(512, 3, padding='same', use_bias=False)\n",
    "        self.norm13 = tf.keras.layers.BatchNormalization()  # skip after this\n",
    "        self.pool13 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.conv14 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm14 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv15 = tf.keras.layers.Conv2D(512, 1, padding='same', use_bias=False)\n",
    "        self.norm15 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv16 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm16 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv17 = tf.keras.layers.Conv2D(512, 1, padding='same', use_bias=False)\n",
    "        self.norm17 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv18 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm18 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv19 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm19 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv20 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm20 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv21 = tf.keras.layers.Conv2D(64, 1, padding='same', use_bias=False)  # apply on skipped connection\n",
    "        self.norm21 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv22 = tf.keras.layers.Conv2D(1024, 3, padding='same', use_bias=False)\n",
    "        self.norm22 = tf.keras.layers.BatchNormalization()\n",
    "        # Feature Extractor Ends Here!\n",
    "        \n",
    "        # Detector Layer!\n",
    "        self.conv23 = tf.keras.layers.Conv2D(NUM_ANCHORS*(4+1+NUM_OBJECTS), 1, padding='same')\n",
    "    \n",
    "    def load_pretrained_weights(self, dir_weights):\n",
    "        for idx_layer, layer in enumerate(self.layers):\n",
    "            if idx_layer == 49:  # skip final layer beacause of class mismatch\n",
    "                continue\n",
    "            filename = dir_weights + '/' + str(idx_layer)\n",
    "            with open(filename, 'rb') as file:\n",
    "                weights = pickle.load(file)\n",
    "            \n",
    "            layer.set_weights(weights)\n",
    "        \n",
    "        print('Weights loaded.')\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # imgs.shape = [B, IMG_H, IMG_W, 3]\n",
    "        \n",
    "        # for now, resize and reshape imgs to vector\n",
    "        imgs = tf.image.resize_images(imgs, [416, 416])\n",
    "        \n",
    "        x = self.conv1(imgs)\n",
    "        x = self.norm1(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.norm5(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.norm6(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.norm7(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = self.norm8(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x = self.pool8(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = self.norm9(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv10(x)\n",
    "        x = self.norm10(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = self.norm11(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv12(x)\n",
    "        x = self.norm12(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv13(x)\n",
    "        x = self.norm13(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        x_skip = tf.identity(x)\n",
    "        x = self.pool13(x)\n",
    "        \n",
    "        x = self.conv14(x)\n",
    "        x = self.norm14(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv15(x)\n",
    "        x = self.norm15(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv16(x)\n",
    "        x = self.norm16(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv17(x)\n",
    "        x = self.norm17(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv18(x)\n",
    "        x = self.norm18(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv19(x)\n",
    "        x = self.norm19(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x = self.conv20(x)\n",
    "        x = self.norm20(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        \n",
    "        x_skip = self.conv21(x_skip)\n",
    "        x_skip = self.norm21(x_skip)\n",
    "        x_skip = tf.nn.leaky_relu(x_skip, alpha=0.1)\n",
    "        x_skip = tf.space_to_depth(x_skip, block_size=2)  # lossless shrinkage of feature map\n",
    "        \n",
    "        x = tf.concat([x_skip, x], axis=-1)  # low_level features concatenated with high_level features\n",
    "        \n",
    "        x = self.conv22(x)\n",
    "        x = self.norm22(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.1)\n",
    "        # Feature Extractor ends here!\n",
    "        \n",
    "        # Detector layer\n",
    "        x = self.conv23(x)\n",
    "        \n",
    "        # reshape output\n",
    "        pred = tf.reshape(x, [-1, GRID_H, GRID_W, NUM_ANCHORS, 4+1+NUM_OBJECTS])\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def get_loss(self, predictions, labels):\n",
    "        # predictions.shape = [BATCH_SIZE, GRID_H, GRID_W, NUM_ANCHORS, 5+NUM_OBJECTS] (they are in grid space)\n",
    "        # labels.shape = [BATCH_SIZE, GRID_H, GRID_W, NUM_ANCHORS, 6]\n",
    "\n",
    "        # apply corresponding transformations on predictions\n",
    "        predictions_yx, predictions_hw, predictions_prob_obj, predictions_prob_class = apply_transformations(predictions)\n",
    "\n",
    "        # map predictions_bbox to [0,1] space\n",
    "        predictions_yx, predictions_hw = grid2normalized(predictions_yx, predictions_hw)\n",
    "\n",
    "        # map labels_bbox to [0,1] space\n",
    "        labels_yx, labels_hw = grid2normalized(labels[...,0:2], labels[...,2:4])\n",
    "\n",
    "        # get ground truth bboxes using labels_bbox & prob_obj in labels\n",
    "        labels_bbox = tf.concat([labels_yx, labels_hw], axis=-1)\n",
    "        bboxes_gt = tf.map_fn(get_boxes_gt, (labels_bbox, labels[...,5]), dtype=tf.float32)\n",
    "\n",
    "        # compute iou scores for each anchor in each grid for all bboxes_gt\n",
    "        iou_scores = get_iou_scores(predictions_yx, predictions_hw, bboxes_gt)\n",
    "\n",
    "        # keep anchors whose iou_scores are above THRESHOLD_IOU_SCORES\n",
    "        iou_scores_best = tf.reduce_max(iou_scores, axis=4, keep_dims=True)\n",
    "        iou_mask = tf.cast(iou_scores_best > THRESHOLD_IOU_SCORES, tf.float32)\n",
    "\n",
    "        ## Loss\n",
    "        # object confidence loss (presence and absence)\n",
    "        loss_confidence = get_confidence_loss(labels[...,5:6], iou_mask, predictions_prob_obj)\n",
    "\n",
    "        # classification loss\n",
    "        loss_classification = get_classification_loss(labels[...,5:6], labels[...,4], predictions_prob_class)\n",
    "\n",
    "        # regression loss\n",
    "        predictions_bbox = tf.concat([predictions_yx, predictions_hw], axis=-1)\n",
    "        loss_regression = get_regression_loss(labels_bbox, predictions_bbox, labels[...,5:6])\n",
    "\n",
    "        # total loss\n",
    "        loss = ( loss_confidence + loss_classification + loss_regression ) / tf.cast(tf.shape(labels)[0], tf.float32)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self, dataset):\n",
    "        '''trains the model for one epoch'''\n",
    "        epoch_loss = tf.constant(0.)\n",
    "        for idx_batch, data in enumerate(tfe.Iterator(dataset)):\n",
    "            with tfe.GradientTape() as tape:\n",
    "                # forward pass\n",
    "                predictions = self.forward(data[0])\n",
    "                \n",
    "                # reverse x & y axis\n",
    "                predictions = tf.concat([predictions[...,1::-1], predictions[...,3:1:-1], predictions[...,4:]], axis=-1)\n",
    "        \n",
    "                # compute loss\n",
    "                loss = self.get_loss(predictions, data[1])\n",
    "                \n",
    "            # backward pass (compute gradients)\n",
    "            gradients = tape.gradient(loss, self.variables)\n",
    "            \n",
    "            # update parameters\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(gradients, self.variables), \n",
    "                global_step=tf.train.get_or_create_global_step()\n",
    "            )\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            print('Batch:', '{}/{}'.format(idx_batch, NUM_BATCHES), '| Loss=', loss.numpy(), '\\t', end='\\r')\n",
    "\n",
    "        return (epoch_loss/(idx_batch+1)).numpy()\n",
    "        \n",
    "    def predict(self, imgs):\n",
    "        '''predicts bboxes and draws them on the image'''\n",
    "        # imgs.shape = [B, IMG_H, IMG_W, 3]\n",
    "        \n",
    "        # forward pass\n",
    "        predictions = self.forward(imgs)\n",
    "        \n",
    "        predictions = tf.concat([predictions[...,1::-1], predictions[...,3:1:-1], predictions[...,4:]], axis=-1)\n",
    "        \n",
    "        # post-process to get bounding boxes\n",
    "        outputs = predictions2outputs(predictions)  \n",
    "        # CAUTION!!!\n",
    "        # TODO: use batch multi-class nms (currently works with BATCH_SIZE=1)\n",
    "        # reference: https://github.com/tensorflow/models/blob/master/research/object_detection/core/post_processing.py\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset processing\n",
    "dataset_train = tf.data.TFRecordDataset(DATA_TRAIN)\n",
    "dataset_train = dataset_train.map(parse_record)\n",
    "dataset_train = dataset_train.shuffle(buffer_size=1024)\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model = Model()\n",
    "#     for data in tfe.Iterator(dataset_train):\n",
    "#         model.forward(data[0])\n",
    "#         break\n",
    "#     model.load_pretrained_weights('../weights_YOLO')\n",
    "    checkpoint = tfe.Checkpoint(model=model, optimizer_step=tf.train.get_or_create_global_step())\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43adb35137004fa7bc9d149cc45d192f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss=0.0103102764114737514436\n",
      "checkpoint saved.\n",
      "Epoch:1 | Loss=0.0103185987100005157085\n",
      "Epoch:2 | Loss=0.0102623915299773228446\n",
      "Epoch:3 | Loss=0.0102505879476666457275\n",
      "Epoch:4 | Loss=0.0101879872381687164274\n",
      "Epoch:5 | Loss=0.0102306194603443154774\n",
      "checkpoint saved.\n",
      "Epoch:6 | Loss=0.0101803708821535115425\n",
      "Epoch:7 | Loss=0.0101903667673468597394\n",
      "Epoch:8 | Loss=0.0102683082222938547236\n",
      "Epoch:9 | Loss=0.0101447794586420065784\n",
      "Epoch:10 | Loss=0.010109278373420238654\n",
      "checkpoint saved.\n",
      "Epoch:11 | Loss=0.010020317509770393795\n",
      "Epoch:12 | Loss=0.009986557997763157036\n",
      "Epoch:13 | Loss=0.009998523630201817144\n",
      "Epoch:14 | Loss=0.009994677267968655996\n",
      "Epoch:15 | Loss=0.009973772801458836984\n",
      "checkpoint saved.\n",
      "Epoch:16 | Loss=0.009969739243388176796\n",
      "Epoch:17 | Loss=0.010015984065830708224\n",
      "Epoch:18 | Loss=0.009976127184927464454\n",
      "Epoch:19 | Loss=0.009973480366170406846\n",
      "Epoch:20 | Loss=0.009992413222789764476\n",
      "checkpoint saved.\n",
      "Batch: 1379/4068 | Loss= 0.000208619795\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5ae4f2233cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:{} | Loss={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bbf4db411bd1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# backward pass (compute gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualEnv/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    762\u001b[0m     grad = imperative_grad.imperative_grad(\n\u001b[1;32m    763\u001b[0m         \u001b[0m_default_vspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualEnv/lib/python3.5/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[0;32m---> 65\u001b[0;31m         tape._tape, vspace, target, sources, output_gradients, status)  # pylint: disable=protected-access\n\u001b[0m",
      "\u001b[0;32m~/virtualEnv/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[0;34m(*orig_outputs)\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0morig_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     result = _magic_gradient_function(op_name, attrs, num_inputs,\n\u001b[0;32m--> 141\u001b[0;31m                                       op_inputs, op_outputs, orig_outputs)\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       print(\"Gradient for\", op_name, \"inputs\", op_inputs, \"output_grads\",\n",
      "\u001b[0;32m~/virtualEnv/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_magic_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    515\u001b[0m       nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    516\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualEnv/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m         padding, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1244\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.optimizer = tf.train.AdamOptimizer(1e-6)\n",
    "    for i in tqdm_notebook(range(30)):\n",
    "        loss = model.train(dataset_train)\n",
    "        print('Epoch:{} | Loss={}'.format(i, loss))\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            # save checkpoint\n",
    "            checkpoint.save(file_prefix=CHECKPOINT_PREFIX)\n",
    "            print('checkpoint saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset processing\n",
    "dataset_test = tf.data.TFRecordDataset(DATA_TRAIN)\n",
    "dataset_test = dataset_test.map(parse_record)\n",
    "dataset_test = dataset_test.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on dataset\n",
    "with tf.device('/gpu:0'):\n",
    "    # load trained model\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "    \n",
    "    for idx_img, data in enumerate(tfe.Iterator(dataset_test)):\n",
    "        # predict\n",
    "        output = model.predict(data[0])\n",
    "        \n",
    "        # write images\n",
    "        img_out = draw_output(data[0][0].numpy(), output[0].numpy())\n",
    "        cv2.imwrite(DIR_IMG_OUT+ '/'+str(idx_img)+'.png', cv2.cvtColor(img_out, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        if idx_img > 50:\n",
    "            break\n",
    "#         img = imgs_out.numpy()[0]\n",
    "#         img = (img * 255).astype(np.uint8)\n",
    "#         cv2.imwrite(DIR_IMG_OUT+ '/'+str(idx_img)+'.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on frame sequence\n",
    "\n",
    "def process_img(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_W, IMG_H))\n",
    "    img = (img / 255.).astype(np.float32)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # load trained model\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "    \n",
    "    filenames = sorted(os.listdir('test_input1/in'))\n",
    "    for filename in filenames:\n",
    "        path = './test_input1/in' + '/' + filename\n",
    "        \n",
    "        img = cv2.imread(path)\n",
    "        img = process_img(img)\n",
    "\n",
    "        output = model.predict(img)\n",
    "\n",
    "        # write images\n",
    "        img_out = draw_output(img[0], output[0].numpy())\n",
    "\n",
    "        cv2.imwrite('test_output1'+ '/'+filename, cv2.cvtColor(img_out, cv2.COLOR_RGB2BGR))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on video\n",
    "def process_img(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_W, IMG_H))\n",
    "#     img = cv2.flip(img, 0)\n",
    "    img = (img / 255.).astype(np.float32)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "IDX_VIDEO = 5\n",
    "DIR_IMGS_OUT = 'test_output/{}'.format(IDX_VIDEO)\n",
    "if not os.path.exists(DIR_IMGS_OUT):\n",
    "    os.makedirs(DIR_IMGS_OUT)\n",
    "with tf.device('/gpu:0'):\n",
    "    # load trained model\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "    \n",
    "    video_name = 'test_videos/{}.mp4'.format(IDX_VIDEO)\n",
    "    vidcap = cv2.VideoCapture(video_name)\n",
    "    success,img = vidcap.read()\n",
    "    success = True\n",
    "    idx_img = 0\n",
    "    while success:\n",
    "        img = process_img(img)\n",
    "\n",
    "        output = model.predict(img)\n",
    "\n",
    "        # write images\n",
    "        img_out = draw_output(img[0], output[0].numpy())\n",
    "        cv2.imwrite('{}/{:04d}'.format(DIR_IMGS_OUT, idx_img) + '.jpg', cv2.cvtColor(img_out, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        success,img = vidcap.read()\n",
    "        idx_img += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
